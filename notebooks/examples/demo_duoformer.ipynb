{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Refactored DuoFormer: Multi-Scale Vision Transformer for Medical Imaging\n",
    "\n",
    "This notebook demonstrates the refactored DuoFormer architecture - a multi-scale vision transformer designed for **general medical image classification**. DuoFormer combines the power of convolutional neural networks (CNNs) for feature extraction with multi-scale attention mechanisms for improved performance across various medical imaging modalities.\n",
    "\n",
    "## üìä Key Features\n",
    "\n",
    "- **Multi-Scale Attention**: Processes features at multiple resolutions simultaneously\n",
    "- **Hybrid Architecture**: Combines ResNet backbone with Vision Transformer\n",
    "- **General Medical Imaging**: Works with histopathology, radiology, dermatology, ophthalmology, and more\n",
    "- **Flexible Configuration**: Supports various backbone architectures and scales\n",
    "- **Platform-Agnostic**: Auto-detects hardware (CUDA/MPS/CPU) and optimizes settings\n",
    "\n",
    "> **Original Work**: [xiaoyatang/duoformer_TCGA](https://github.com/xiaoyatang/duoformer_TCGA) | **Paper**: [arXiv:2506.12982](https://arxiv.org/abs/2506.12982)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and set up the environment.\n",
    "\n",
    "### üì¶ Dependency Management\n",
    "\n",
    "This project uses `pip-tools` for reproducible dependency management:\n",
    "\n",
    "- `requirements.in`: Lists direct dependencies with flexible version constraints\n",
    "- `requirements.txt`: Auto-generated lockfile with pinned versions and hashes\n",
    "- `setup_environment.py`: Automated setup script that handles the entire process\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- ‚úÖ Reproducible environments across different machines\n",
    "- ‚úÖ Automatic conflict resolution\n",
    "- ‚úÖ Security through dependency hashing\n",
    "- ‚úÖ Easy updates while maintaining compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Install required packages using automated setup\n",
    "# This script handles pip-tools, requirements compilation, and installation\n",
    "# Run this cell once at the start of your session\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if setup_environment.py exists\n",
    "setup_script = Path(\"setup_environment.py\")\n",
    "\n",
    "if setup_script.exists():\n",
    "    print(\"üöÄ Running automated environment setup...\")\n",
    "    print(\"This will:\")\n",
    "    print(\"  1. Upgrade pip and install pip-tools\")\n",
    "    print(\"  2. Compile requirements.in to requirements.txt\")\n",
    "    print(\"  3. Install all dependencies with conflict resolution\")\n",
    "    print(\"  4. Validate the environment\")\n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "    # Run the setup script\n",
    "    result = subprocess.run(\n",
    "        [sys.executable, str(setup_script), \"--verbose\"], capture_output=False\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n‚úÖ Environment setup completed successfully!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Setup encountered an error. Please check the output above.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  setup_environment.py not found!\")\n",
    "    print(\"Installing dependencies directly...\")\n",
    "    subprocess.run(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"torch\",\n",
    "            \"torchvision\",\n",
    "            \"timm\",\n",
    "            \"einops\",\n",
    "            \"matplotlib\",\n",
    "            \"numpy\",\n",
    "            \"pillow\",\n",
    "            \"tqdm\",\n",
    "            \"scikit-learn\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Alternative: Manual installation (uncomment if needed)\n",
    "\n",
    "\n",
    "# !pip install torch torchvision timm einops matplotlib numpy pillow tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Set up paths (platform-agnostic)\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "\n",
    "# Configure device (automatically detects best available)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(f\"üîß Using device: {device}\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"   Running on CPU (CUDA not available)\")\n",
    "    print(\"   Note: Training will be slower without GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import DuoFormer Models\n",
    "\n",
    "Let's import the DuoFormer architecture and explore its components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DuoFormer components\n",
    "from models import (\n",
    "    build_model,\n",
    "    build_model_no_extra_params,\n",
    "    build_hybrid,\n",
    "    count_parameters,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DuoFormer modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Create Sample Medical Image Dataset\n",
    "\n",
    "For demonstration purposes, we'll create a synthetic medical image dataset. In practice, you would replace this with your actual medical imaging data (e.g., X-rays, CT scans, MRI images).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Synthetic medical image dataset for demonstration.\n",
    "    In practice, replace this with actual medical imaging data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_samples: int = 1000,\n",
    "        num_classes: int = 10,\n",
    "        image_size: int = 224,\n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "    ):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform or self._default_transform()\n",
    "\n",
    "        # Generate synthetic data (replace with actual data loading)\n",
    "        np.random.seed(42)\n",
    "        self.images = np.random.randn(num_samples, 3, image_size, image_size).astype(\n",
    "            np.float32\n",
    "        )\n",
    "        self.labels = np.random.randint(0, num_classes, num_samples)\n",
    "\n",
    "        # Simulate different medical image patterns\n",
    "        for i in range(num_samples):\n",
    "            pattern_type = self.labels[i] % 4\n",
    "            if pattern_type == 0:  # Circular patterns (tumors)\n",
    "                self._add_circular_pattern(i)\n",
    "            elif pattern_type == 1:  # Linear patterns (fractures)\n",
    "                self._add_linear_pattern(i)\n",
    "            elif pattern_type == 2:  # Diffuse patterns (inflammation)\n",
    "                self._add_diffuse_pattern(i)\n",
    "            else:  # Complex patterns\n",
    "                self._add_complex_pattern(i)\n",
    "\n",
    "    def _default_transform(self):\n",
    "        \"\"\"Default augmentation pipeline for medical images.\"\"\"\n",
    "        return transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def _add_circular_pattern(self, idx: int):\n",
    "        \"\"\"Simulate tumor-like circular patterns.\"\"\"\n",
    "        center = np.random.randint(50, 174, 2)\n",
    "        radius = np.random.randint(10, 30)\n",
    "        y, x = np.ogrid[: self.image_size, : self.image_size]\n",
    "        mask = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= radius**2\n",
    "        self.images[idx, 0, mask] += np.random.uniform(0.5, 1.5)\n",
    "\n",
    "    def _add_linear_pattern(self, idx: int):\n",
    "        \"\"\"Simulate fracture-like linear patterns.\"\"\"\n",
    "        start = np.random.randint(0, self.image_size, 2)\n",
    "        end = np.random.randint(0, self.image_size, 2)\n",
    "        rr, cc = np.linspace(start[0], end[0], 100).astype(int), np.linspace(\n",
    "            start[1], end[1], 100\n",
    "        ).astype(int)\n",
    "        valid = (rr >= 0) & (rr < self.image_size) & (cc >= 0) & (cc < self.image_size)\n",
    "        self.images[idx, 1, rr[valid], cc[valid]] += np.random.uniform(0.5, 1.5)\n",
    "\n",
    "    def _add_diffuse_pattern(self, idx: int):\n",
    "        \"\"\"Simulate inflammation-like diffuse patterns.\"\"\"\n",
    "        noise = np.random.randn(self.image_size, self.image_size) * 0.3\n",
    "        from scipy.ndimage import gaussian_filter\n",
    "\n",
    "        smoothed = gaussian_filter(noise, sigma=5)\n",
    "        self.images[idx, 2] += smoothed\n",
    "\n",
    "    def _add_complex_pattern(self, idx: int):\n",
    "        \"\"\"Simulate complex medical patterns.\"\"\"\n",
    "        self._add_circular_pattern(idx)\n",
    "        self._add_linear_pattern(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        image = self.images[idx].transpose(1, 2, 0)  # CHW -> HWC for PIL\n",
    "        image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(\n",
    "            np.uint8\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.from_numpy(self.images[idx])\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "print(\"‚úÖ Medical image dataset class created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "full_dataset = MedicalImageDataset(\n",
    "    num_samples=500, num_classes=10, image_size=224  # Reduced for demo\n",
    ")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_size = int(0.7 * len(full_dataset))\n",
    "val_size = int(0.15 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset Statistics:\")\n",
    "print(f\"   Training samples: {len(train_dataset)}\")\n",
    "print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "print(f\"   Test samples: {len(test_dataset)}\")\n",
    "print(f\"   Batch size: {batch_size}\")\n",
    "print(f\"   Number of classes: {10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Visualize Sample Medical Images\n",
    "\n",
    "Let's visualize some sample images from our dataset to understand the data better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples: int = 6):\n",
    "    \"\"\"Visualize sample medical images from the dataset.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(min(num_samples, len(dataset))):\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        # Denormalize image for visualization\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.numpy()\n",
    "\n",
    "        if image.shape[0] == 3:  # CHW format\n",
    "            image = image.transpose(1, 2, 0)\n",
    "\n",
    "        # Clip and normalize for display\n",
    "        image = np.clip(image, 0, 1) if image.max() <= 1 else image / 255.0\n",
    "\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f\"Class: {label}\", fontsize=10)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Sample Medical Images\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_samples(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Initialize DuoFormer Model\n",
    "\n",
    "Now let's create and configure the DuoFormer model with different settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "config = {\n",
    "    \"depth\": 12,  # Number of transformer blocks\n",
    "    \"embed_dim\": 384,  # Embedding dimension\n",
    "    \"num_heads\": 6,  # Number of attention heads\n",
    "    \"num_classes\": 10,  # Number of output classes\n",
    "    \"num_layers\": 2,  # Number of scales (2, 3, or 4)\n",
    "    \"num_patches\": 49,  # Number of patches (7x7)\n",
    "    \"proj_dim\": 384,  # Projection dimension\n",
    "    \"mlp_ratio\": 4.0,  # MLP expansion ratio\n",
    "    \"attn_drop_rate\": 0.1,  # Attention dropout rate\n",
    "    \"proj_drop_rate\": 0.1,  # Projection dropout rate\n",
    "    \"freeze_backbone\": False,  # Whether to freeze ResNet backbone\n",
    "    \"backbone\": \"r50\",  # Backbone architecture ('r50' or 'r18')\n",
    "    \"pretrained\": True,  # Use pretrained backbone weights\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = build_model_no_extra_params(**config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "trainable_params, total_params = count_parameters(model)\n",
    "\n",
    "print(\"üîç Model Architecture Summary:\")\n",
    "print(f\"   Model type: DuoFormer\")\n",
    "print(f\"   Backbone: ResNet-{config['backbone'][1:]}\")\n",
    "print(f\"   Number of scales: {config['num_layers']}\")\n",
    "print(f\"   Embedding dimension: {config['embed_dim']}\")\n",
    "print(f\"   Number of heads: {config['num_heads']}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:.2f}M\")\n",
    "print(f\"   Total parameters: {total_params:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Pipeline\n",
    "\n",
    "Let's implement a training pipeline with proper metrics tracking and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Training pipeline for DuoFormer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(\n",
    "            model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "        }\n",
    "\n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(data)\n",
    "            loss = self.criterion(output, target)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(\n",
    "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100.*correct/total:.2f}%\"}\n",
    "            )\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def validate(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "            for data, target in progress_bar:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "                progress_bar.set_postfix(\n",
    "                    {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100.*correct/total:.2f}%\"}\n",
    "                )\n",
    "\n",
    "        avg_loss = total_loss / len(val_loader)\n",
    "        accuracy = 100.0 * correct / total\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 10):\n",
    "        \"\"\"Train the model for multiple epochs.\"\"\"\n",
    "        best_val_acc = 0.0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nüìÖ Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.history[\"train_loss\"].append(train_loss)\n",
    "            self.history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate(val_loader)\n",
    "            self.history[\"val_loss\"].append(val_loss)\n",
    "            self.history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"   Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"   Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"model_state_dict\": self.model.state_dict(),\n",
    "                        \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                        \"val_acc\": val_acc,\n",
    "                    },\n",
    "                    \"best_duoformer_model.pt\",\n",
    "                )\n",
    "                print(f\"   ‚úÖ Best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "        print(f\"\\nüéâ Training complete! Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "        return self.history\n",
    "\n",
    "\n",
    "print(\"‚úÖ Trainer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Train the Model\n",
    "\n",
    "Now let's train our DuoFormer model on the medical image dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(model=model, device=device, learning_rate=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Train the model\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "history = trainer.fit(\n",
    "    train_loader=train_loader, val_loader=val_loader, epochs=5  # Reduced for demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Visualize Training Progress\n",
    "\n",
    "Let's plot the training and validation metrics to understand the model's learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict):\n",
    "    \"\"\"Plot training and validation metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Plot loss\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "    ax1.plot(epochs, history[\"train_loss\"], \"b-\", label=\"Training Loss\", linewidth=2)\n",
    "    ax1.plot(epochs, history[\"val_loss\"], \"r-\", label=\"Validation Loss\", linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Loss\", fontsize=12)\n",
    "    ax1.set_title(\"Model Loss\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, history[\"train_acc\"], \"b-\", label=\"Training Accuracy\", linewidth=2)\n",
    "    ax2.plot(epochs, history[\"val_acc\"], \"r-\", label=\"Validation Accuracy\", linewidth=2)\n",
    "    ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "    ax2.set_title(\"Model Accuracy\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Highlight best validation accuracy\n",
    "    best_epoch = np.argmax(history[\"val_acc\"])\n",
    "    best_acc = history[\"val_acc\"][best_epoch]\n",
    "    ax2.scatter(best_epoch + 1, best_acc, color=\"green\", s=100, zorder=5)\n",
    "    ax2.annotate(\n",
    "        f\"Best: {best_acc:.2f}%\",\n",
    "        xy=(best_epoch + 1, best_acc),\n",
    "        xytext=(best_epoch + 1, best_acc - 5),\n",
    "        arrowprops=dict(arrowstyle=\"->\", color=\"green\"),\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Model Evaluation on Test Set\n",
    "\n",
    "Let's evaluate our trained model on the test set and compute various metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            _, predicted = output.max(1)\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_probs = np.array(all_probs)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (all_predictions == all_targets).mean() * 100\n",
    "\n",
    "    print(\"\\nüìä Test Set Performance:\")\n",
    "    print(f\"   Overall Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Classification report\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    class_names = [f\"Class {i}\" for i in range(10)]\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_targets, all_predictions, target_names=class_names, digits=3\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "    plt.title(\"Confusion Matrix\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "    plt.ylabel(\"True Label\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy, all_predictions, all_targets, all_probs\n",
    "\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"best_duoformer_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# Evaluate\n",
    "test_acc, predictions, targets, probabilities = evaluate_model(\n",
    "    model, test_loader, device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Feature Visualization\n",
    "\n",
    "Let's visualize the learned features and attention maps to understand what the model has learned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_maps(model, image_tensor, device):\n",
    "    \"\"\"Visualize attention maps from the model.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Hook to capture attention weights\n",
    "    attention_weights = []\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        if hasattr(output, \"detach\"):\n",
    "            attention_weights.append(output.detach())\n",
    "\n",
    "    # Register hooks on attention layers\n",
    "    hooks = []\n",
    "    for name, module in model.named_modules():\n",
    "        if \"attn\" in name.lower() and hasattr(module, \"forward\"):\n",
    "            hook = module.register_forward_hook(hook_fn)\n",
    "            hooks.append(hook)\n",
    "            if len(hooks) >= 3:  # Limit to 3 attention layers\n",
    "                break\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "        _ = model(image_tensor)\n",
    "\n",
    "    # Remove hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    # Visualize\n",
    "    if attention_weights:\n",
    "        fig, axes = plt.subplots(1, min(3, len(attention_weights)), figsize=(15, 5))\n",
    "        if len(attention_weights) == 1:\n",
    "            axes = [axes]\n",
    "\n",
    "        for idx, attn in enumerate(attention_weights[:3]):\n",
    "            if attn.dim() >= 2:\n",
    "                # Take mean across heads if multi-head attention\n",
    "                if attn.dim() > 2:\n",
    "                    attn_map = attn.mean(dim=0 if attn.shape[0] > 1 else 1)\n",
    "                else:\n",
    "                    attn_map = attn\n",
    "\n",
    "                # Ensure 2D for visualization\n",
    "                if attn_map.dim() > 2:\n",
    "                    attn_map = attn_map.view(attn_map.shape[0], -1)\n",
    "\n",
    "                axes[idx].imshow(\n",
    "                    attn_map.cpu().numpy(), cmap=\"hot\", interpolation=\"nearest\"\n",
    "                )\n",
    "                axes[idx].set_title(f\"Attention Layer {idx+1}\", fontsize=10)\n",
    "                axes[idx].axis(\"off\")\n",
    "\n",
    "        plt.suptitle(\"Attention Maps Visualization\", fontsize=14, fontweight=\"bold\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No attention weights captured.\")\n",
    "\n",
    "\n",
    "# Get a sample image\n",
    "sample_image, sample_label = next(iter(test_loader))\n",
    "visualize_attention_maps(model, sample_image[0], device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Model Inference\n",
    "\n",
    "Let's demonstrate how to use the trained model for inference on new medical images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(model, image_tensor, device, class_names=None):\n",
    "    \"\"\"Make prediction on a single image.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prepare image\n",
    "        if image_tensor.dim() == 3:\n",
    "            image_tensor = image_tensor.unsqueeze(0)\n",
    "        image_tensor = image_tensor.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(image_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "\n",
    "        # Get prediction\n",
    "        confidence, predicted_class = probabilities.max(1)\n",
    "\n",
    "    predicted_class = predicted_class.item()\n",
    "    confidence = confidence.item()\n",
    "\n",
    "    # Get top 3 predictions\n",
    "    top3_prob, top3_classes = probabilities[0].topk(3)\n",
    "\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class {i}\" for i in range(probabilities.shape[1])]\n",
    "\n",
    "    return {\n",
    "        \"predicted_class\": predicted_class,\n",
    "        \"class_name\": class_names[predicted_class],\n",
    "        \"confidence\": confidence,\n",
    "        \"top3\": [\n",
    "            (class_names[c.item()], p.item()) for c, p in zip(top3_classes, top3_prob)\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on sample images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(6):\n",
    "    image, true_label = test_dataset[i]\n",
    "\n",
    "    # Make prediction\n",
    "    result = predict_single_image(model, image, device)\n",
    "\n",
    "    # Visualize\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        img_display = image.numpy().transpose(1, 2, 0)\n",
    "    else:\n",
    "        img_display = image\n",
    "\n",
    "    img_display = (\n",
    "        np.clip(img_display, 0, 1) if img_display.max() <= 1 else img_display / 255.0\n",
    "    )\n",
    "\n",
    "    axes[i].imshow(img_display)\n",
    "    axes[i].set_title(\n",
    "        f\"True: Class {true_label}\\n\"\n",
    "        f\"Pred: {result['class_name']} ({result['confidence']:.2%})\",\n",
    "        fontsize=10,\n",
    "        color=\"green\" if result[\"predicted_class\"] == true_label else \"red\",\n",
    "    )\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Model Predictions on Test Images\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Model Export and Deployment\n",
    "\n",
    "Finally, let's prepare the model for deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX format for deployment\n",
    "def export_to_onnx(model, save_path=\"duoformer_model.onnx\"):\n",
    "    \"\"\"Export model to ONNX format.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 3, 224, 224, device=device)\n",
    "\n",
    "    # Export\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        save_path,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    )\n",
    "    print(f\"‚úÖ Model exported to {save_path}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "# Export the model\n",
    "# onnx_path = export_to_onnx(model)  # Uncomment to export\n",
    "\n",
    "\n",
    "# Save model with metadata\n",
    "model_metadata = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"input_size\": (3, 224, 224),\n",
    "    \"num_classes\": 10,\n",
    "    \"normalization\": {\"mean\": [0.485, 0.456, 0.406], \"std\": [0.229, 0.224, 0.225]},\n",
    "    \"performance\": {\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"best_val_accuracy\": max(history[\"val_acc\"]),\n",
    "    },\n",
    "}\n",
    "\n",
    "torch.save(model_metadata, \"duoformer_final_model.pt\")\n",
    "print(\"‚úÖ Model saved with metadata for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary and Conclusions\n",
    "\n",
    "In this notebook, we demonstrated:\n",
    "\n",
    "1. **üîß Setup**: Configured the DuoFormer architecture with multi-scale attention\n",
    "2. **üìä Data**: Created a synthetic medical image dataset with various patterns\n",
    "3. **üß† Model**: Initialized DuoFormer with ResNet backbone and Vision Transformer\n",
    "4. **üéØ Training**: Trained the model with proper metrics tracking\n",
    "5. **üìà Evaluation**: Evaluated performance with confusion matrix and classification report\n",
    "6. **üîç Visualization**: Visualized attention maps and learned features\n",
    "7. **üöÄ Deployment**: Prepared the model for production deployment\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- DuoFormer effectively combines CNN and Transformer architectures\n",
    "- Multi-scale attention enables better feature extraction at different resolutions\n",
    "- The model is particularly suitable for medical imaging tasks\n",
    "- Flexible configuration allows adaptation to various dataset requirements\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Fine-tune** on your specific medical imaging dataset\n",
    "2. **Experiment** with different backbone architectures (ResNet-18, ResNet-101)\n",
    "3. **Adjust** the number of scales based on your image resolution\n",
    "4. **Implement** domain-specific augmentations for medical images\n",
    "5. **Deploy** using ONNX or TorchScript for production inference\n",
    "\n",
    "---\n",
    "\n",
    "For questions or contributions, please refer to the [DuoFormer GitHub repository](https://github.com/duoformer/duoformer).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
